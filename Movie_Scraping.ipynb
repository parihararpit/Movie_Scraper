{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Movie_Scraping.ipynb","provenance":[],"mount_file_id":"1ljjFvZSRreZ1b9c-bNt1nsweGu2w3Szb","authorship_tag":"ABX9TyNWScf83r43kfvX/ye+fogq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OZ5J0WjHRoGS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600534483285,"user_tz":-330,"elapsed":3271,"user":{"displayName":"ARPIT PARIHAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcFq47ZfvCyt0U56PTxmBxtoQxi0-f79G49p6vwQ=s64","userId":"10464704970609957460"}}},"source":["from urllib.request import urlopen as uReq\n","from bs4 import BeautifulSoup as soup\n","import csv\n","\n","\n","\n","############################################################\n","\n","# online\n","# Movies of five consecutive years from 2020-2016\n","movies_link = [\"https://www.imdb.com/list/ls071285764/\"]\n","\n","cur_dir = '/content/drive/My Drive/ArtificialIntelligence_project'\n","out_file = open( cur_dir + '/dataset/movies.csv', 'w', newline='')\n","\n","\n","#write index\n","words  = ['title', 'reviews_link']\n","writer = csv.writer(out_file)\n","writer.writerow(words)\n","out_file.close()\n","\n","for movies in movies_link:\n","  u_client = uReq(movies) # open url in code/memory html\n","  html_file = u_client.read()\n","  u_client.close() # not needed anymore\n","\n","  # list of all movies titles\n","  bsoup = soup(html_file, \"html.parser\") \n","  lst = bsoup.findAll(\"h3\", {\"class\": \"lister-item-header\"})\n","\n","  cur_dir = '/content/drive/My Drive/ArtificialIntelligence_project'\n","  out_file = open( cur_dir + '/dataset/movies.csv', 'a', newline='')\n","\n","\n","  for item in lst:\n","      title_tag = item.find(\"a\", href = True)\n","      title = title_tag.text.strip()\n","      link = 'https://www.imdb.com' + title_tag['href'].split('?')[0] + 'reviews?ref_=tt_urv'\n","      if not title or not link:\n","          continue\n","      \n","      words  = [title, link]\n","      writer = csv.writer(out_file)\n","      writer.writerow(words)\n","  out_file.close()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"hw1vR9-6RXaw","colab_type":"code","colab":{}},"source":["!apt-get update\n","!apt install chromium-chromedriver #read about\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","!pip install selenium #read about"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mz7IvRTBSErc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600534494501,"user_tz":-330,"elapsed":1093,"user":{"displayName":"ARPIT PARIHAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcFq47ZfvCyt0U56PTxmBxtoQxi0-f79G49p6vwQ=s64","userId":"10464704970609957460"}}},"source":["from selenium import webdriver\n","\n","from urllib.request import urlopen as uReq\n","\n","# read BeautifulSoup https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n","from bs4 import BeautifulSoup as soup\n","import csv\n","import time"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JMfTTTGR8Js","colab_type":"code","colab":{}},"source":["import csv\n","cur_dir='/content/drive/My Drive/ArtificialIntelligence_project'\n","out_file = open( cur_dir + '/dataset/reviews.csv', 'w', newline='', encoding=\"utf-8\")\n","k = 1\n","\n","head  = ['imdb_link', 'review_title', 'review_text', 'rating']\n","writer = csv.writer(out_file) # read about csv module in python https://www.geeksforgeeks.org/working-csv-files-python/\n","writer.writerow(head)\n","\n","data_path = cur_dir + '/dataset/movies.csv'\n","movies_list = open(data_path).read().split('\\n')\n","movies_list=[]\n","with open(data_path,'r') as rf:\n","  reader = csv.reader(rf, delimiter=',')\n","  for row in reader:\n","    movies_list.append(row[1])\n","############## prepare browser simulation #############################\n","\n","# prepare the option for the chrome driver\n","# options = webdriver.ChromeOptions()\n","# options.add_argument('headless')\n","\n","# browser = webdriver.Chrome(chrome_path)\n","\n","# set options to be headless, ..\n","\n","# read about ChromeOptions https://chromedriver.chromium.org/capabilities\n","options = webdriver.ChromeOptions() \n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","\n","# open it, go to a website, and get results\n","wd = webdriver.Chrome('chromedriver',options=options)\n","\n","# use we.get(url) to get a web page with js in wd\n","############## browser is in wd #############################\n","\n","# ignore first row, as it has header only\n","for movie in movies_list[1:len(movies_list)-1]:\n","    reviews_link=movie\n","    print(reviews_link + \"\\n\")\n","\n","    wd.get(reviews_link)\n","\n","    try:\n","        load_button = wd.find_elements_by_xpath(\"//*[@id='load-more-trigger']\")[0]\n","        for i in range(2):\n","            load_button.click()\n","            time.sleep(3)\n","    except:\n","        print('Load Button Not Found!!')\n","        continue\n","\n","    html_file = wd.page_source\n","\n","    bsoup = soup(html_file, \"html.parser\")\n","    #bsoup = bsoup.encode(\"utf-8\")\n","    lst = bsoup.findAll(\"div\", {\"class\": \"lister-item-content\"})\n","\n","    for item in lst:\n","\n","        title_tag = item.find(\"a\", {\"class\": \"title\"})\n","        rating_tag = item.find(\"span\", {\"class\": \"rating-other-user-rating\"})\n","        review_tag = item.find('div', {'class': 'text show-more__control'})\n","\n","        # if any field is not available ignore this review\n","        if not rating_tag or not review_tag or not title_tag:\n","            continue\n","\n","        imdb_link = reviews_link\n","        review_title = title_tag.text.strip()\n","        review_text = review_tag.text.strip()\n","        rating = rating_tag.findChildren('span')[0].text.strip()\n","\n","        # verify output\n","        print(\"\\n\\nReview page: \" + imdb_link\n","            + \"\\nReview title: \" + review_title\n","            + \"\\nReview text: \" + review_text\n","            + \"\\nRating: \" + rating)\n","        # break\n","\n","\n","        words = [imdb_link, review_title, review_text, rating]\n","        writer = csv.writer(out_file)\n","        writer.writerow(words)\n","        \n","        # break\n","        \n","    # waiting for few seconds, so that imdb does not \n","    # recognise our system as bot and does not block our ip\n","    time.sleep(3)\n","    # break\n","    \n","        \n","out_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PIyO-sLSSMsn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600534638664,"user_tz":-330,"elapsed":5110,"user":{"displayName":"ARPIT PARIHAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcFq47ZfvCyt0U56PTxmBxtoQxi0-f79G49p6vwQ=s64","userId":"10464704970609957460"}}},"source":["\n","import csv\n","\n","\n","cur_dir = '/content/drive/My Drive/ArtificialIntelligence_project'\n","\n","reviews_data = open(cur_dir + '/dataset/reviews.csv', 'r', newline='', encoding=\"utf-8\")\n","dataset = open(cur_dir + '/dataset/datasets.csv', 'w', newline='', encoding=\"utf-8\")\n","\n","reviews_writer = csv.writer(dataset)\n","reviews_reader = csv.reader(reviews_data, delimiter=',')\n","\n","# write index\n","words  = ['review_title', 'review', 'rating']\n","reviews_writer.writerow(words)\n","\n","# print(\"Dataset:\")\n","k = 1\n","\n","for line in reviews_reader:\n","    if k==1:\n","        k = k+1\n","        continue\n","    words = review_title, review_text, rating = line[1:4]\n","    reviews_writer.writerow(words)\n","\n","    # print first 10 reviews, for verification\n","    # if k<10:\n","    #     print(\"\\n\\nReview title: \" + review_title\n","    #           + \"\\nReview text: \" + review_text\n","    #           + \"\\nRating: \" + rating)\n","    # k=k+1\n","\n","reviews_data.close()\n","dataset.close()"],"execution_count":7,"outputs":[]}]}